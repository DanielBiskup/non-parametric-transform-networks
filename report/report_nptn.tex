% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
%\RequirePackage[hyphens]{url}

\usepackage{gensymb}

\usepackage{wrapfig,lipsum,booktabs}
\usepackage{biblatex}
\bibliography{references.bib}

%#######BEGIN SPACE SAVING MAGIC######################################
%##########PRODUCES SOME ERRORS BUT SEEMS TO WORK!!!##################
%\iftrue  % Use this line if you want space magical saving  (7.75 = 8 pages)
%
\iffalse   % Use this line if you dont want to save space   (8.5 = 9 pages)

%% This is taken from: https://gist.github.com/yig/81b4c993ea13252edc81
%% Make everything look better.
%% http://tex.stackexchange.com/questions/553/what-packages-do-people-load-by-default-in-latex
%% http://www.howtotex.com/packages/9-essential-latex-packages-everyone-should-use/
\usepackage{microtype}

%% Shrink space around figures.
%% This beats manually adding negative \vspace commands everywhere.
\setlength{\textfloatsep}{0pt}
\setlength{\textfloatsep}{20pt plus 2pt minus 4pt}
\setlength{\textfloatsep}{10pt plus 2pt minus 4pt}
\setlength{\textfloatsep}{10pt plus 1pt minus 2pt}
\setlength{\dbltextfloatsep}{3pt}
\setlength{\intextsep}{5pt}
\setlength{\abovecaptionskip}{5pt}
\setlength{\belowcaptionskip}{3pt}
\setlength{\parskip}{4pt}
% around equations
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setlength\abovedisplayshortskip{3pt}
\setlength\belowdisplayshortskip{3pt}

%% The wrapfigure environment is also a good way to use less space for a small figure.
%% It is finicky.
%% See: http://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions#Wrapping_text_around_figures

%% Shrink space around enumerate and itemize
%% From: http://tex.stackexchange.com/questions/10684/vertical-space-in-lists
%\usepackage{enumitem}
%% Set it globally:
% \setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
% \setlist{itemsep=1pt, topsep=3pt}
%% Or locally:
% \begin{enumerate}[itemsep=1pt, topsep=12pt, partopsep=0pt]

%% Less space around titles.
%% From: http://tex.stackexchange.com/questions/4999/change-whitespace-above-and-below-a-section-heading
%\usepackage[medium,compact]{titlesec}
%% or
%\usepackage{titlesec}
%\titlespacing*{\section}{0pt}{*1}{*1}

%% Shrink the \paragraph command.
%\renewcommand{\paragraph}[1]{\noindent {\bf #1}}

%% Allow Latex to use a smaller minimum spacing between lines.
%\renewcommand{\baselinestretch}{0.97}
%\renewcommand{\baselinestretch}{0.982}

%% Shrink space between lines. You probably want to prefer baselinestretch in general.
%% From: http://tex.stackexchange.com/questions/23824/6-lines-in-one-inch
%% 6 lines in one inch with an 11pt font:
%\linespread{0.901}
%\linespread{0.909}
%\linespread{1}

%% Shrink the spacing between lines in just the bibliography. This is good.
% \usepackage{setspace}

%% Then wrap the bibliography in a \begin{spacing}. You can wrap any section like this.
% \begin{spacing}{0.9}
% \bibliographystyle{...}
% \bibliography{references}
% \end{spacing}

%##Reduce space arount section, subsection and subsubsection headings.
\usepackage{lipsum}
\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
%##Reduce space below figures
% https://tex.stackexchange.com/a/23315
\setlength{\belowcaptionskip}{-10pt}
\fi
%#######END SPACE SAVING MAGIC######################################

\begin{document}
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Non-Parametric Transformation Networks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Non-Parametric Transformation Networks \\Lab: Cognitive Robotics}
%
\titlerunning{Non-Parametric Transformation Networks}  % abbreviated title (for running head)
%  
%
\author{Daniel Biskup and Catherine Capellen
 \\Supervisors: Arul Selvam and Max Schwarz}
%
\authorrunning{Daniel Biskup and Catherine Capellen} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Daniel Biskup and Catherine Capellen}
%
\institute{University of Bonn}

\maketitle              % typeset the title of the contribution
\begin{abstract} Non-Parametric Transformation Networks(NPTNs) are generalizations of Convolutional Neural Networks designed to learn invariance towards transformations of the data. 
We implemented experiments described in the paper by Pal and Savvides \cite{NPTN18} and also propose a network architecture called RotNet designed to be rotation invariant. Our experiments show a small performance advantage of the NPTNs, but a worse performance then found by \cite{NPTN18}.

\end{abstract}
\section{Introduction}
Convolutional Neural Networks (CNNs) are state of the art in image classification tasks [FIND CITE]. Certain kind of transformations on these images, as small rotations or translations, do not affect the class of the depicted object.  
Using augmented data in training as well as choices in the architecture of CNNs as pooling layers, aim to improve the robustness of CNNs to specific transformations. 
Pooling is a method that is usually performed spatially, to obtain invariance to small translations. 

Non-Parametric Transformation Networks (NPTNs) are adaptions of CNNs, which use pooling to achieve invariance to other transformations by pooling over layers, rather than separately on the different layers.
Pal and Savvides \cite{NPTN18} introduced NPTNs and performed experiments to test NPTNs against CNNs. The main objective of our project was to verify the results from their paper. 
Inspired by the NPTNs we additionally designed a network architecture, called RotNet, supposed to be invariant towards rotations.

In this paper we will first describe the network architectures in more detail, then describe our implementation and experiments and lastly our results and the conclusions.

\section{Networks Architectures} In the following we will introduce Convolutional Neural Networks (CNNs), Non-Parametric Transformation Networks (NPTNs) and a custom network architecture we call RotNet.

\subsection{Convolutional Neural Networks}
NPTNs can be seen as a generalization of CNNs. For that reason we will briefly discuss CNNs. 
\subsubsection{Convolution}
As the name suggests, CNNs make heavy use of the convolution operation. Convolutions
%, as illustrated in Figure \ref{fig:conv},
 calculate the weighted sum over a sub region of an input image in order to calculate the pixel-values of the output. The weights used for computing the weighted sum are given by a matrix called filter or kernel. In Computer Vision they are used to detect different kinds of patterns, since convolving an image with a kernel will produce high responses at the regions that match the pattern the filter is designed to respond to. Thus one kernel might respond to vertical edges, another  one to horizontal edges and yet another one to yet another pattern. While kernels traditionally got designed by experts in Computer Vision, within CNNs they are expected to be learned by the network.


\subsubsection{Spatial Max Pooling}
Spatial max pooling, generally just called max pooling, is another common operation used in CNNs.
Each neuron in the max pooling layer takes into account only a subregion of the previous
layers output and passes on the maximum value. By doing this it makes the network invariant to small translations. 
%Max pooling is illustrated in Figure \ref{fig:maxpooling}.



\subsection{Parametric and Non-Parametric Transformation Networks}

Parametric Transformation Networks (PTNs) and Non-Parametric Transformation Networks (NPTNs), as introduced in \cite{NPTN18}, try to add more transformation invariance to CNNs. For this a transformation group G is used:
Where CNNs have one filter, PTNs and NPTNs have $|G|$ filters and a pixel wise max pooling operation is performed over the filters, such that for each pixel the strongest result of the convolution with a filter is used. This is illustrated in Figure \ref{pic:nptn}.


\begin{wrapfigure}{r}{0.5\textwidth}
%\begin{figure}
	\begin{center}
	\includegraphics[scale=0.15]{result_images/nptn_paper.png}
	\caption{Illustration of CNNs vs. NPTNs. The structure of the NPTNs is the same as for PTNs. (Figure taken from \cite{NPTN18})}
	\label{pic:nptn}
	\end{center}
\end{wrapfigure}
%\end{figure}
For the PTNs these $|G|$ filters are transformations of a base filter with weights $w$. The transformations belong to a unitary group of transformations G and are learned simultaneously to $w$ for each of the different $w$. 


For NPTNs the filters are independent of each other and are not restricted to belong to a transformation group. However they are expected to learn transformations.
For $|G|=1$ a NPTN conceptually does the same as a CNN, which is why NPTNs can be considered to be a generalization of CNNs. 



\subsection{RotNets}
Rotation Networks (RotNets) were our idea for the additional task of designing a network that, due to its structure, is invariant to rotations. 
The idea is based on the same structure as PTNs (and NPTNs), just that the group of transformations is fixed to be rotations. Therefore there exists one set of weights and this weights are rotated into the used kernels. 
\newcommand{\pytorch}{PyTorch }
\section{Implementation in \pytorch}
\newcommand{\nnaffinegrid}{\nolinkurl{nn.functional.affine\textunderscore grid} }
\newcommand{\nngridsample}{\nolinkurl{nn.functional.grid\textunderscore sample} }
\newcommand{\nnConvFunction}{\nolinkurl{nn.functional.conv2d} }
\newcommand{\nnConvLayer}{\nolinkurl{nn.Conv2d} }
\newcommand{\nnMaxPool}{\nolinkurl{nn.MaxPool3d} }
\newcommand{\nnAvgPool}{\nolinkurl{nn.AvgPool3d} }
\newcommand{\nnTensor}{\nolinkurl{nn.Tensor} }
\newcommand{\nnParameter}{\nolinkurl{torch.nn.Parameter} }

As the authors of the paper on NPTNs \cite{NPTN18} we used \pytorch for the implementation of the experiments.
To be able to reproduce the experiments performed in the paper and conduct experiments on RotNet we had to implement the NPTN layer as well as the RotNet layer.

When implementing new network architectures in \pytorch it's crucial to specify which values AutoGrad should view as parameters of the network in order for it to be able to apply backpropagation algorithm to the network. Furthermore you need to take care, that all calculations and operations performed in the forward path are differentiable by AutoGrad.
\subsection{The NPTN Layer}
%TODO Add NPTN graphics!
Our implementation follows the description given in the NPTN paper. A sketch of our implementation is shown in Figure \ref{fig:NptnImplementation}.
Lets say we have $M$ inputs and want $N$ outputs while using a set of $|G|$ different filters for each path from input to output.
Just as for CNNs, all filters we use during convolution are also parameters that are expected to be learned by the network.
For this reason we can perform the convolutions by using an instance of the \nnConvLayer layer which will automatically initialize the filters and also register them as learnable parameters. This way, by using \nnConvLayer we don't need to worry about AutoGrad and the differentiability of parameters ourself.

In it's constructor we specify it to have $M$ inputs and $M \cdot N \cdot |G|$ outputs. The important implementation detail here is, that we need to set the groups option to $M$, which causes \nnConvLayer to use a filter bank of $N\cdot|G|$ filters per input.

%TODO DEL USE SPACE NEEDED__ 
\iffalse
As stated in the \pytorch documentation \cite{PyTorchDocumentation} the output value of the \nnConvLayer layer with input size $(N, C_{in}, H, W)$ and output $(N, C_{out}, H_{out}, W_{out})$ can be precisely described as:
\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*}
but since setting groups to $M$ causes \nnConvLayer to operate as if there are $M$ convolution layers side by side, each seeing one input channel and producing $N\cdot|G|$ output channels, $C_{in} = 1$, which means, that there will be no weighted sum since only one convolution will be calculated per output.
\fi
%TODO __DEL USE SPACE NEEDED

%After calling the convolution, we have a tensor with $N$ consecutive packs of $|G|$ layers produced by the same input. But we need $|G|$ layers of input 1 to be followed by $|G|$ layers of input 2 and so on. This can be achieved by a simple permutation, which we decided to defer until after the max pooling across channels. We will come back to this permutation later.

Next we use \nnMaxPool with stride 1 and kernel size $(|G|,1,1)$. Note, that this does not resemble spatial max pooling as is commonly used in CNNs, but rather the pixel wise maximum of $|G|$ consecutive layers.

%Following this, to perform the aforementioned permutation, we defined a matrix of numbers from zero to the number of layers left in the tensor, permuted it, and then indexed the tensor with it. Note, that this is valid, because indexing tensors is tracked by AutoGrad.
The resulting tensor needs to be reordered in such a way, that we always get a sequence of $M$ outputs, non stemming from the same input. We need $N$ of those sequences stacked on top of each other.
%TODO Maybe add: Permutation with matrix as formula.

This is followed by the average pooling across layers, which is common to CNNs and archived by using \nnAvgPool.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{result_images/NptnImplementation.png}
		\caption{A forward pass through the NPTN layer with M input channels and N output channels.}
		\label{fig:NptnImplementation}
	\end{center}
\end{figure}

\subsection{The RotNet Layer}
Within the implementation of the NPTN layer we were able to use an instance of \nnConvLayer, because every kernel used for convolution was also a learnable parameter of the network.
For the RotNet layer on the other hand, we only wanted to learn the unrotated kernels. So we couldn't use \nnConvLayer class but needed to use the \nnConvFunction  function instead.
This required us to define and initialize a \nnTensor representing the unrotated kernels. We also needed to mark them as trainable parameters which is done by wrapping the \nnTensor into an \nnParameter object.\\
To compute the rotated versions of the kernels, we used the functions \nnaffinegrid and \nngridsample, because they are differentiable by AutoGrad while performing interpolation which allows for arbitrary rotation angles to be applied. We had to specify the rotation matrices we want to use, convert them to flow fields using \nnaffinegrid and then perform the rotations using \nngridsample. The constructor of our RotNet layer expects a parameter \nolinkurl{alpha} which tells the layer to create rotated versions of the kernel with rotation angles between \nolinkurl{-alpha} and \nolinkurl{alpha}.\\
The rest of the layer implementation, namely the \nnMaxPool, permutation of layers and the final \nnAvgPool are the same as in our NPTN implementation.

%\subsection{Multi Layer Networks}


\subsubsection{Input size to fully connected layer}
The last layer of a network hierarchy usually is a fully connected layer, and \pytorch requires us to specify the dimensions of this layer on instantiation. We decided to expect the input size of the network as an argument to the networks constructor, initialize a random tensor with this specified input size, and then pass it through the network excluding the fully connected layer, in order to use the size of the result to initialize the fully connected layer. This makes it easier to change the network (e.g. kernel size, input image size, number of layers) without having to calculate and adapt the size of the fully connected layer manually. 

\iffalse
\subsubsection{YAML files for Experiment setup}
We had to run a lot of experiments. In order to avoid code duplication and promote code readability we parsed YAML files containing the parameters.
\fi

\section{Experiments and Results}	
In the following we will present three experiments from the paper that we tried to reproduce. 
All the experiments used a two-layered or three-layered structure. From now on (as in the paper) when we use the term layer, we refer to a structure that contains either the convolution layer, the NPTN layer or the RotNet layer, followed by other operations. The concrete design of the two-layered network is shown in Figure \ref{pic:network_structure}. The last log-softmax layer is not described in the paper, but was added by us. 
For the experiments we noticed in general that the runtime of NPTNs of RotNets was alike and slightly longer than for the CNNs. This is not surprising as the NPTNs contain the convolution operation as well as additional operations at the same place that the CNNs only perform the convolution. 

%\begin{figure}
\begin{wrapfigure}{r}{0.5\textwidth}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/network_structure.jpg}
	\caption{Structure of a two layered network.}
	\label{pic:network_structure}
	\end{center}
\end{wrapfigure}
%\end{figure}

\subsection{Experiments on CIFAR-10}
The first experiment used the CIFAR-10 dataset \cite{CIFAR}. A CNN and several NPTNs with different values for $|G|$ are tested. The dimension of the NPTNs is changed for the different values of G such that all networks have the same amount of filters.
The paper describes in detail the setup of the experiments regarding learning rate, data preprocessing and network dimensions. However other important information as filter size and loss function is missing. 
For the filter size we used 5, since for the other experiments, the paper mentions  filter sizes of 3, 5 or 7 where 5 often shows the best performance for the NPTNs. 
In Figure \ref{pic:first_experiment} we show the results from the paper next to our experimental results. For the loss function we chose Negative Log Likelihood Loss (NLLL). 
Comparing the loss values of the networks to each other, in the paper, all NPTNs perform better than the CNN. In our results, only one of the NPTNs performs better.
However the loss values we obtained were to different to the ones from the paper to perform a direct comparison. 
Also we are not sure if we used the same loss as in the paper.  Therefore we also calculated the accuracy of the networks. The values are consistent with the loss for this experiment, so for each network if the loss is lower, then the accuracy is higher compared to the other networks.

One thing we noticed during this and also the following experiment, is that the test loss was consistently lower than the training loss and also the test accuracy was higher than the training accuracy, even though we used different datasets and used the same function to calculate training and test measures.

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/experiment1.jpg}
	\caption{Comparison of the networks on the CIFAR dataset, left: results from \citep{NPTN18}, right: our results}
	\label{pic:first_experiment}
	\end{center}
\end{figure}


\section{Rotation Experiments on MNIST}
The second experiment from the paper compares a CNN and several NPTNs. Again the dimensions are chosen in a way that matches the values in the paper and leads to the same amount of filters for all networks. 
This experiment uses the MNIST dataset \cite{MNIST} where data augmentation was applied to all images from training and test set by randomly rotating the images up to a certain angle $\rho$. 
\begin{wraptable}{r}{7cm}
%\begin{table}
\begin{center}
\begin{tabular}{ |c| c c || c c| } 
 \hline 
  & \multicolumn{2}{|c||}{Paper results} & \multicolumn{2}{|c|}{Our results} \\
 \hline
 Rotations & \hspace{1mm} 60$\degree$ \hspace{1mm} & \hspace{1mm} 90$\degree$  & \hspace{1mm} 60$\degree$ \hspace{2mm} & \hspace{2mm}90$\degree$ \hspace{1mm} \\
 \hline
 CNN(36) & 0.066 & 0.106 & 0.0560 &  0.0736 \\ 
 %NPTN(36,1) & 0.064 & 0.108  & & \\ 
 NPTN(18,2) & 0.053 & 0.092  & n.a. & 0.0739\\ 
 NPTN(12,3) & 0.055 & 0.087 & 0.0409 &  0.0726\\
 \hline
RN(12,3,$\alpha$=30) & & &  0.0768 & 0.1176 \\
RN(12,3,$\alpha$=60) & & & 0.0888 & 0.1353 \\
RN(12,3,$\alpha$=90) & & & 0.0882 & 0.1347 \\

\hline

\end{tabular}

 \caption{Loss of different networks on the MNIST dataset with random rotations up to the shown degree. Our loss: Negative Log Likelihood loss, paper loss: unknown}
\end{center}
\label{tab:resultsexp2}
%\end{table}
\end{wraptable}
The paper shows the results for different values of $\rho$. We chose to run the experiments which used the highest values for $\rho$, since the performance difference between NPTNs and CNNs was the biggest for those. Also we added RotNets for comparison.
The Paper did specify the data preprocessing in detail, but didn't give new instructions for the learning rate, so we kept the same experiment setup as in the first experiment.
The results from our experiments are shown in Table \ref{tab:resultsexp2}. 
%[TODO: GRAPHIC OR TABLE WILL BE INSERTED HERE]

The experiments in the paper show the best results for the NPTNs with parameters $|G|=2$ or $|G|=3$. Our results show the that these values are also better than the CNN. The difference in loss is also quite similar to the one in the paper. RotNet performs worse than CNN and NPTNs. Accuracy and loss again were consistent. 

\section{Three-layered networks}
This experiment from the paper compares three-layered NPTNs and CNNs on the CIFAR-10 \cite{CIFAR} dataset. 
The network dimensions are not described well and the paper states that the amount of parameters for those networks is similar, but not exactly the same as in the previous experiments. 
Therefore we determined possible dimensions for the amount of channels in the NPTNs and CNNs. 
Again no new learning rate was given for this experiment, so we initially tried with the same experimental setup as in the first two experiments, but using this the networks overfit greatly and the loss values did not improve. 
Therefore we reduced the learning rate and achieved a result, which had less generalization error. 
Different than in the first two experiments accuracy and loss are not consistent anymore. While the loss starts getting worse after some iteration, the accuracy stays approximately stable and we therefore used the accuracy to compare the networks.

The paper achieves a lower test loss compared to the experiment with the two-layered network. Also the NPTNs with a higher value of $|G|$ have a bigger improvement compared to their baseline CNN. Our results show the opposite: All NPTNs perform worse than the CNN and they perform worse, the bigger $|G|$ is.
So many parameters on how to perform this experiment needed to be set by us, since they were not mentioned in the paper, that it is not comparable to the paper anymore.

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/experiment3_2.jpg}
	\caption{Accuracy of the three-layered networks on  the CIFAR-10 dataset.}
	\label{pic:experiment3}
	\end{center}
\end{figure}

\section{Experiments for RotNet}
The final experiment was designed by us to see if RotNet was able to perform one of the ideas behind its design: the ability to be invariant to rotations without learning them.
% Maybe say that earlier too! [HELLO]
For this we copied the setup of the MNIST rotation experiment with the difference, that this time we only rotated the test set. The results are shown in Figure \ref{pic:experiment4}. 


\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/experiment4.jpg}
	\caption{Results of the experiment designed for testing the ability of the networks to handle rotations that only appear in the test set.}
	\label{pic:experiment4}
	\end{center}
\end{figure}



\section{Discussion and further work [FURTHER WORK?}
Some of the NPTNs perform better than the CNN. However the difference is not as prominent as in the paper. Since the run time of the NPTNs is slightly higher than that of the CNNs, CNNs might be able to achieve better results in the same runtime.

Overall it was difficult to compare our results to the paper, because important information was missing. 
RotNet performed worse than the NPTNs and the CNNs in all experiments.
It also did not show the expected advantages for handling rotated data, without learning these rotations first. 

%WHY WE THINK NPTNS ARE LESS POWERFULL WHY ROTNETS AREWORSE THAN NPTNs WHY IT IS MAYBE NOT A GOOD IDEA TO RO-TATE EARLY FILTERs

%\section{What we learned}
Personallly we learned in this lab about CNNs and the 
Dealing with tensor dimensions.
Implement networks architectures using \pytorch.

% ---- Bibliography ----
%TODO References to add:
% Figures
%   Convolution: https://arxiv.org/abs/1603.07285v2
\small
\printbibliography
\end{document}
