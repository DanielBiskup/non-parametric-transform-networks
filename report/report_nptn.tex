% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
%\RequirePackage[hyphens]{url}

%\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Non-Parametric Transformation Networks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Non-Parametric Transformation Networks \\Lab: Cognitive Robotics}
%
\titlerunning{Non-Parametric Transformation Networks}  % abbreviated title (for running head)
%  
%
\author{Daniel Biskup and Catherine Capellen
 \\Supervisors: Arul Selvam and Max Schwarz}
%
\authorrunning{Daniel Biskup and Catherine Capellen} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Daniel Biskup and Catherine Capellen}
%
\institute{University of Bonn}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The abstract 
\end{abstract}
%

\section{Introduction}
Convolutional Neural Networks (CNNs) are state of the art in image classification tasks [FIND CITE]. Certain kind of transformations on these images do not affect the class of the depicted object. 
Examples for this are small rotations, translations or perspective transforms. [Other kind of transformations might change the object class, for example if you rotate the image of the number 6, it will look like a 9.] 
Using augmented data in training as well as choices in the architecture of CNNs as pooling layers, aim to improve the robustness of CNNs to specific transformations. 
Pooling is a method that is usually performed spatially, to obtain invariance to small translations. 
Non-Parametric Transformation Networks (NPTNs) are extensions of CNNs, which use Pooling to achieve invariance to other transformations by pooling over layers, rather than seperately on the different layers.
Pal and Savvides \cite{NPTN} introduced NPTNs and performed experiments to test NPTNs against CNNs. The main objective of our project was to verify the results from the paper. 
[Task: design rotational additional netowrk] ditionally we propose our own Architecture RotNet, which is designed to be invariant to Rotations and will also describe the experiments we performed with RotNet. 
[TODO]
Inspired by the NPTNs we designed a network architecture,which we calle RotNet, supoused to be invariant specificaly towards rotations.

First we will describe the network architectures in more detail in Section [REF], then describe our implemenation [REF] and lastly our Results [REF] and Conclusions [REF].

GENERAL TOPIC: Object recognition, CNNs, POOLING, PTN/NPTN, Paper results (SHORT) 
TASK
ROTNET TASK

[IDEA: DO A SMALL PARAGRAPH ABOUT PAPER; SPATIAL TRANSFORMER NETWORK; Methods how to deal with transformations

\section{Networks Architectures}
The kinds of networks this report is concerned with are Convolutional Neural Networks (CNNs), Non-Parametric Transformation Networks (NPTNs), a custom network architecture we call RotNets.

\subsection{Convolutional Neural Networks}
NPTNs can be seen as a generalization of CNNs. For that reason we will briefly discuss CNNs. 
\subsubsection{Convolution}
As the name suggests, NPTNs make heavy use of the Convolution Operation. Convolutions, as illustraded in Figure \ref{fig:conv}, calculate the weighted sum over a sub region of an input image in order to calculate the pixel-values of the output. The weights used for computing the weighted sum are given by a matrix called filter or kernel. In Computer Vision they are used to detect different kinds of patterns, since convolving an image with a kernel will produce high responses at the regions that match the pattern the filter is designed to respond to. Thus one kernel might respond to vertical edges, another  one to horizontal edges and yet another one to yet another pattern. While kernels traditionally got designed by experts in Computer Vision, within CNNs they are expected to be learned by the network.
\begin{figure}
	\begin{center}
		\includegraphics[width=\textwidth]{result_images/Conv.png}
		\caption{Convolving a 3 $\times$ 3 kernel over a 4 $\times$ 4 input using unit strides [SOURCE]}
		\label{fig:conv}
	\end{center}
\end{figure}

\subsubsection{Spatial Max Pooling}
Another common operation used in CNNs is Max Pooling.
Each neuron in the max pooling layer takes into account only a subregion of the previous
layers output and takes the maximum value of it to be it's output. By doing this it makes the network invariant to small translations. Max pooling is illustrated in Figure \ref{fig:maxpooling}. [MAYBE WHY SPATIAL]

\begin{figure}
	\begin{center}
		\includegraphics[width=0.40\textwidth]{result_images/maxpooling.jpg}
		\caption{Example of Max Pooling over a 4 $\times$ 4 image with a window size of $2 \times 2$ and a stride of $2$ [MORE]}
		\label{fig:maxpooling}
	\end{center}
\end{figure}

\subsection{Parametric and Non-Parametric Transformation Networks}

Paramateric Transformation Networks (PTNs) and Non-Parametric Transformation Networks, as introduced in \cite{NPTN} try to add more tranformation invariance to CNNs. For this Max Pooling over layers is used. [DIFFERENCE TO SPATIAL]
[Introduce group G]
Where a CNN has one filter, PTNs and NPTNs have $|G|$ filters and a pixel wise max pooling operation is performed over the filters, such that for each pixel the strongest result of the convolution with a filter is used. [IMAGE]

For the PTNs these $|G|$ filters are transformations of a base filter with weights $w$. The transformations belong to a (unitary) group of transformations and are learned simultaniously to $w$ for each of the different $w$. [MAYBE use TN nodes for explanation?]

[NPTNs fitler independent]

For $|G|=1$ a NPTN conceptually does the same as a CNN, which is why NPTNs can be considered to be a generalization of CNNs. 


\subsection{RotNets}
Rotation Networks (RotNets) were our idea for the additional task of designing a network that due to its structure is invariant to rotations. 
The idea is based on the same structure as PTNs (and NPTNs), just that the group of transformations is fixed to be rotations. Therefore there exists one set of weights $w$, which is learned for each $|G|$ transformations and this weights are rotated into the new kernels. 
[ALPHA!]

\section{Implementation in PyTorch}
As the authors of the paper on NPTNs [REF] we used PyTorch for the implemenation of the experiments.
To be able to reproduce the experiments performed in the paper and conduct experiments on RotNet we had to implement the NPTN layer as well as the RotNet layer.

When implementing new network architectures in PyTorch it's crucial to specify which values AutoGrad should view as parameters of the network in order for it to be able to apply backpropagation algorithm to the network. Furthermore you need to take care, that all calculations and operations performed in the forward path are differentiable by AutoGrad.
\subsection{The NPTN Layer}
%TODO Add NPTN graphics!
Our implementation follows the description given in the NPTN paper.
Lets say we have M inputs and want N outputs while using a set of G different filters for each path from input to output.
Just as for CNNs, all filters we use during convolution are also parameters that are expected to be learned by the network.
For this reason we can perform the convolutions by using an instance of the nn.Conv2d layer which will automatically initialize the filters and also register them as learn able parameters. This way, by using nn.Conv2d we don't need to worry about AutoGrad and differentiability of parameters ourself.

In it's constructor we specify it to have M inputs and M*N*G outputs. The important implementation detail here is, that we need to set the groups option to M, which causes Conv2d to use a filter bank of N*G filters per input.

%TODO DEL USE SPACE NEEDED__ 
As stated in the PyTorch documentation [REF] the output value of the nn.Conv2d layer with input size $(N, C_{in}, H, W)$ and output $(N, C_{out}, H_{out}, W_{out})$ can be precisely described as:
\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*}
but since setting groups to M causes nn.Conv2d to operate as if there are M conv layers side by side, each seeing one input channel and producing N*G output channels, $C_{in} = 1$, which means, that there will be no weighted sum since only one convolution will be calculated per output.
%TODO __DEL USE SPACE NEEDED

After calling the convolution, we have a tensor with N consecutive packs of G layers produced by the same input. But we need G layers of input 1 to be followed by G layers of input 2 and so on. This can be achieved by a simple permutation, which we decided to defer until after the max pooling across channels. We will come back to this permutation later.

Next we use nn.MaxPool3d with stride 1 and kernel size (G,1,1). Note, that this does not resemble spatial max pooling as is commonly used in CNNs, but rather the pixel wise maximum of G consecutive layers.

Following this, to perform the aforementioned permutation, we defined a matrix of numbers from 0 to the number of layers left in the tensor, permuted it, and then indexed the tensor with it. Note, that this is valid, because indexing tensors is tracked by AutoGrad.
%TODO Maybe add: Permutation with matrix as formula.

This is followed by the Average Pooling across layers, which is common to CNNs and actually replaces the weighted sum operation in the formula of $\text{out}(N_i, C_{out_j})$ above. This can be archived by using nn.AvgPool3d.

\subsection{The RotNet Layer}
\newcommand{\nnaffinegrid}{\nolinkurl{nn.functional.affine\textunderscore grid} }
\newcommand{\nngridsample}{\nolinkurl{nn.functional.grid\textunderscore sample} }
%\newcommand{\nnaffinegrid}{\url{nn.functional.affine\textunderscore grid}}
%\newcommand{\nngridsample}{\url{nn.functional.grid\textunderscore sample}}
Within the implementation of the NPTN layer we were able to use an instance of nn.Conv2d, because every kernel used for convolution was also a learnable parameter of the network.
For the RotNet layer on the other hand, where we only want to learn the unrotated kernels while performing convolution with a few rotated versions of them, we can't use nn.Conv2d class but need to use the nn.functional.conv2d function instead.
This requires us to define and initialize the unrotated kernels ourself. We also need to mark them as parameters of the network, in order for AutoGrad to be able to calculate the gradients required for back propagation. This is done by wrapping the nn.tensor describing them into an Parameter object.\\
To compute the rotated versions of the kernels, we used the functions \nnaffinegrid and \nngridsample, because they are differentiable by AutoGrad while performing interpolation which allows for arbitrary rotation angles to be applied. We had to specify the rotation matrices we want to use, convert them to flow fields using \nnaffinegrid and then perform the rotations using \nngridsample.\\
The rest of the layer implementation, namely the nn.MaxPool3d, permutation of layers and the final nn.AvgPool3d are the same as in our NPTN implementation.

\subsection{Multi Layer Networks}


\subsubsection{Input size to fully connected layer}
The last layer of a network hierarchy usually is a fully connected layer, and pyTorch requires us to specify the dimensions of this layer on instantiation. We decided to expect the input size of the network as an argument to the networks constructor, initialize a random tensor with this specified input size, and then pass it through the network excluding the fully connected layer, in order to use the size of the result to initialize the fully connected layer. This makes it easier to change the network (e.g. kernel size, input image size, number of layers) without having to calculate and adapt the size of the fully connected layer manually. 

\subsubsection{YAML files for Experiment setup}
We had to run a lot of experiments. In order to avoid code duplication and promote code readability we parsed YAML files containing the parameters.

\section{Experiments and Results}	
In the following we will present three experiments from the paper that we tried to reproduce. 
All the experiments used a two-layered or three-layered structure. [BETTER: From now on when we use the term layer, we are refering to a structure that consist of either Conv Nptn or Rotnet layer, followed by .... ] Before layer referred to the Convolution or NPTN-layers, but here it includes several different operations and  which includes either a NPTN layer, a Convolutional layer or a RotNet layer in each layer. The structure for the two-layered network is shown in Figure \ref{pic:net_structure}. The last softmax layer is not described in the paper, but was added by us [WHY softmax, because it's standard]. 

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/network_structure.jpg}
	\caption{Structure of a two layered network.}
	\label{pic:network_structure}
	\end{center}
\end{figure}

\subsubsection{Determining the size of individual network layers}
[Once the network layers needed are defined, and before useful experiments can be run, one has to stack multiple of them on top of each other to build multi layered networks.]
Unfortunately the NPTN paper didn't specify the size of each layer precisely enough, which forced us to guess and try different values.

The idea is to keep the number of parameters in the networks we want to compare roughly equivalent to allow comparisons between the different architectures performances.

%TODO Fill in more detail here...

\subsection{Experiments on CIFAR10}
The first experiment used the CIFAR10 dataset [CITE]. A CNN and several NPTNs with different values for G are tested. The dimension of the NPTNs is changed for the different values of G such that all networks have the same amount of filters.
The paper describes in detail the setup of the experiments regarding learning rate, data preprocessing and network dimensions. However other important information as filter size and loss function is missing. 
For the filtersize we used 5, since in the other experiments, the paper mentions  filtersizes of 3,5 or 7 and 5 often shows the best performance for the NPTNs. 
In Figure \ref{pic:first_experiment} we show the results from the paper next to our experimental results. For the loss function we chose Negative Log Likelihood Loss (NLLL) [WHY]. 
However we did not get any comparable loss values as in the paper. Comparing the loss values of the networks to each other, in the paper, all NPTNs perform better than the CNN. In our results, only one of the NPTNs performs better.
Since we are not sure if the loss used in the paper is the same, we also calculated the accuracy of the networks. The values are consistent with the loss for this experiment, so for each network if the loss is lower, then the accuracy is higher compared to other networks.

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/experiment1.jpg}
	\caption{The first experiment}
	\label{pic:first_experiment}
	\end{center}
\end{figure}

TRAIN vs TEST ERROR?

\section{Rotation Experiments on MNIST}
The second experiment from the paper compares a CNN and several NPTNs. Again the dimensions are chosen in a way that matches the values in the paper and leads to the same amount of filters for all networks. 
This experiment uses the MNIST dataset [CITE] where all images from training and test set were randomly rotated up to a certain degree. 
The paper shows the results from various maximal rotation degrees. we picked the two strongest rotations, since the performance difference between NPTNs and CNNs was the biggest there and performed our own experiments, adding also RotNets for comparison.
The Paper did specify the data preprocessing in detail, but didn't give new instructions for learning rate, so we kept the same experiment setup as in the first experiment.
The results from our experiments are shown in Figure \ref{pic:second_experiment}. [TODO: new graphic?]


\section{Three-layered-networks}
This experiment from the paper compares three-layered NPTNs and CNNs of the CIFAR10 [CITE] dataset . 
The network dimensions are not described well and the paper states that the amount of parameters for those networks is similar, but not exactly the same as in the previous experiments. 
Therefore we determined possible dimensions for the amount of channels in the NPTNs and CNNs. 
Again no new learning rate was given for this experiment, so we initially tried with the same experimental setup as in the first two experiments, but using this and the dimensions that seemed likely from the paper led the networks to overfit greatly and the loss values did not improve. 
Therefore we reduced the amount of parameters and the learning rate and achieved a result, which had less generalization error. 
Different than in the first two experiments accuracy and loss are not consistent anymore. While the loss starts getting worse after some iteration, the accuracy stays approximately stable and we therefore used the accuracy to compare the networks.
[TODO show results]

\section{Experiments for RotNet}
The final experiment was designed by us to see if RotNet was able to perform one of the ideas behind its design: the ability to be invariant to rotations without learning them. For this we copied the setup of the MNIST rotation experiment with the difference, that this time we only rotated the test set. The results are shown in Figure [FIG]. 


\section{Discussion and further work}
TRAIN vs TEST ERROR?
CONCLUSION: PAPER IS NOT REPRODUCABLE NPTNs FROM THIS
EXPERIMENTS NOT BETTER ( ROTNET BAD

WHY WE THINK NPTNS ARE LESS POWERFULL WHY ROTNETS ARE
WORSE THAN NPTNs WHY IT IS MAYBE NOT A GOOD IDEA TO RO-
TATE EARLY FILTERs

\section{What we learned}
Dealing with tensor dimensions.
Implement networks architectures using PyTorch.

%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
\bibitem[1]{NPTN}
Pal, D. K., \& Savvides, M. (2018). Non-Parametric Transformation Networks. arXiv preprint arXiv:1801.04520.


%TODO References to add:
% Websites
% - PyTorch Documentation: http://pytorch.org/docs/0.3.1/
%
% Papers
% - NPTN paper: https://arxiv.org/abs/1801.04520v3
%
% Figures
%   Convolution: https://arxiv.org/abs/1603.07285v2


\end{thebibliography}
%\clearpage
%\addtocmark[2]{Author Index} % additional numbered TOC entry
%\renewcommand{\indexname}{Author Index}
%\printindex
%\clearpage
%\addtocmark[2]{Subject Index} % additional numbered TOC entry
%\markboth{Subject Index}{Subject Index}
%\renewcommand{\indexname}{Subject Index}
%\input{subjidx.tex}
\end{document}
