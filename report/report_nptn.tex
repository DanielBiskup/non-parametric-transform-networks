% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}

%\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Non-Parametric Transformation Networks} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Non-Parametric Transformation Networks}
%
\titlerunning{Non-Parametric Transformation Networks}  % abbreviated title (for running head)
%  
%
\author{Daniel Biskup and Catherine Capellen}
%
\authorrunning{Daniel Biskup and Catherine Capellen} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Daniel Biskup and Catherine Capellen}
%
\institute{University of Bonn}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The abstract 
\end{abstract}
%

\section{Introduction}
GENERAL TOPIC: Object recognition, CNNs, POOLING, PTN/NPTN, Paper results (SHORT) 
TASK
ROTNET TASK

\section{Related Work}
in lab report ? ( DO WE NEED THIS SECTION?) Maybe mention Spatial-
Transformer networks?

\section{Networks Architectures ( aka Methods)}
The kinds of networks this report is concerned with are Convolutional Neueral Networks (CNNs), Non-Parametric Transformation Networks (NPTNs), a custom network architecture we call RotNets.

\subsection{Convolutional Neueral Networks}
NPTNs can be seen as a generalization of CNNs. For that reason we will briefly discuss CNNs.
\subsubsection{Convolution}
As the name suggests, NPTSn make heavy use of the Convolution Operation. Convolutions calculate the weighted sum over a sub region of an input image in order to calculate the pixel-values of the output. The weights used for calculation the weighted sum are given by a matrix called filter or ‚ÄùKernel. Naturally different Kernels produce different results. In Computer Vision they are used to detect different kinds patterns. Thus one kernel might respond to vertical edges, another  one to horizontal edges and yet another one to yet another pattern. While kernels traditionally got designed by experts in Computer Vision, whithin CNNs they are expected to be learned by the network.
%TODO Add Conv graphic
\subsubsection{Spatial Max Pooling}
Another important operation used in CNNs is Max Pooling.
Each neuron in the max pooling layer connevts to a region of the previous
layer and only take the maximum output. By doing this they make the network
invariant to small translations

\subsection{Non-Parametric Transformation Networks}
\begin{itemize}
  \item	mathmotivation
  \item Architecture
  \item How they are a generalization of CNNs
\end{itemize}

\subsection{RotNets}
Loremipsum

\section{Implementation in with PyTorch}
Because the authors of the paper in NPTNs state, that they used PyTorch for their experiments, and because PyTorch is widely accepted and adopted within the scientific community, we decided to base our experiments on PyTorch as well.
To be able to reproduce the experiments performed in the paper, we had to implement the NPTN layer. To see if our approach, the RotNet, would perform well we had to implement it as well.

When implementing new network architectures in PyTorch it's crucial to specify which values AutoGrad should view as parameters of the network in order for it to be able to apply backpropagation algorithm to the network. Furthermore you need to take care, that all calculations and operations performed in the forward path are differentiable by AutoGrad.
\subsection{The NPTN Layer}
%TODO Add NPTN graphics!
Our implementation follows the description given in the NPTN paper.
Lets say we have M inputs and want N outputs while using a set of G different filters for each path from input to output.
Just as for CNNs, all filters we use during convolution are also parameters that are expected to be learned by the network.
For this reason we can perform the convolutions by using an instance of the nn.Conv2d layer which will automatically initialize the filters and also register them as learn able parameters. This way, by using nn.Conv2d we don't need to worry about AutoGrad and differentiability of parameters ourself.

In it's constructor we specify it to have M inputs and M*N*G outputs. The important implementation detail here is, that we need to set the groups option to M, which causes Conv2d to use a filter bank of N*G filters per input.
%TODO DEL USE SPACE NEEDED__ 
Usually the output value of the nn.Conv2d layer with input size $(N, C_{in}, H, W)$ and output $(N, C_{out}, H_{out}, W_{out})$ can be precisely described as:
\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*}
but since setting groups to M causes nn.Conv2d to operate as if there are M conv layers side by side, each seeing one input channel and producing N*G output channels, $C_{in} = 1$, which means, that there will be no weighted sum since only one convolution will be calculated per output.
%TODO __DEL USE SPACE NEEDED

After calling the convolution, we have a tensor with N consecutive packs of G layers produced by the same input. But we want need that G layers of input 1 are followed by G layers of input 2 and so on. This can be archived by a simple permutation, which we decided to defer until after the max pooling across channels. We will come back to this permutation later.

Next we use MaxPool3d with stride 1 and kernel size (G,1,1). Note, that this does not resemble spatial max pooling as is commonly used in CNNs, but rather the pixel wise maximum of G consecutive layers.

To perform the aforementioned permutation, we defined a matrix of numbers from 0 to the number of layers left in the tensor, permuted it, and then indexed the tensor with it. Note, that this is valid, because indexing tensors is tracked by AutoGrad.
%TODO Maybe add: Permutation with matrix as formula.

This is followed by the Average Pooling across layers, which is common to CNNs. It's replaces the weighted sum operation in the formula of $\text{out}(N_i, C_{out_j})$ above.

This concludes the implementation of the NPTN layer.

But to run actual experiments, you need to stack multiple network layers on
top of each other. And this is where the paper gave us a hard time, because it
didnt specify the size of each layer precisely enough. So we had to try to guess
which filter sizes they used.

\subsection{The RotNet Layer}
DETERMINATION OF NETWORK DIMENSION (48,16): EQUAL AMOUNT
OF PARAMETERS
ROTNET: GRIDSAMPLER, DIFFERENTIABLE
(DIMENSION DING)
NPTN IMPLEMENTATION

\subsubsection{The convolution}
Within the implementation of the NPTN layer we were able to use an instantiation of the nn.Conv2d layer, because we had to learn each kernel we used individually, i.e every kernel used for convolution was also a parameter of the network.\\
For the RotNet layer on the other hand, we need to use the nn.functional.conv2d function which expacts as argument the kernels we want to use, while inside the definition of the NPTN layers class we only "mark" the unrotated kernels as parameters by calling "" on them.

\subsubsection{GRIDSAMPLER, DIFFERENTIABLE}
\subsubsection{DETERMINATION OF NETWORK DIMENSION (48,16): EQUAL AMOUNT OF PARAMETERS}

\subsection{More implementation details}
\subsubsection{Input size to fully connected layer}
Can be calculated dynamically.
\subsubsection{YAML files for Experiment setup}

4 EXPERIMENTS
- DESIGN (WHAT WAS IN PAPER AND WHAT WASN'T)



\section{Experiments and Results}	
In the following we will present three experiments from the paper that we tried to reproduce and tried to mimic as closely as possible. 
All the experiments use a two-layered or three-layered structure which include either a NPTN layer, a Convolutional layer or a RotNet layer in each layer. The structure for the two-layered network is shown in Figure \ref{pic:net_structure}. The last softmax layer is not described in the paper, but was added by us [WHY softmax, because it's standard]. 

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/network_structure.jpg}
	\caption{Structure of a two layered network.}
	\label{pic:network_structure}
	\end{center}
\end{figure}

\subsection{Experiments on CIFAR10}
The first experiment used the CIFAR10 dataset [CITE]. A CNN and several NPTNs with different values for G are tested. The dimension of the NPTNs is changed for the different values of G such that all networks have the same amount of filters.
The paper describes in detail the setup of the experiments regarding learning rate, data preprocessing and network dimensions. However other important information as filter size and loss function is missing. 
For the filtersize we used 5, since in the other experiments, the paper mentions  filtersizes of 3,5 or 7 and 5 often shows the best performance for the NPTNs. 
In Figure \ref{pic:first_experiment} we show the results from the paper next to our experimental results. For the loss function we chose Negative Log Likelihood Loss (NLLL) [WHY]. 
However we did not get any comparable loss values as in the paper. Comparing the loss values of the networks to each other, in the paper, all NPTNs perform better than the CNN. In our results, only one of the NPTNs performs better.
Since we are not sure if the loss used in the paper is the same, we also calculated the accuracy of the networks. The values are consistent with the loss for this experiment, so for each network if the loss is lower, then the accuracy is higher compared to other networks.

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.35]{result_images/experiment1.jpg}
	\caption{The first experiment}
	\label{pic:first_experiment}
	\end{center}
\end{figure}

TRAIN vs TEST ERROR?

\section{Rotation Experiments on MNIST}
The second experiment from the paper compares a CNN and several NPTNs. Again the dimensions are chosen in a way that matches the values in the paper and leads to the same amount of filters for all networks. 
This experiment uses the MNIST dataset [CITE] where all images from training and test set were randomly rotated up to a certain degree. 
The paper shows the results from various maximal rotation degrees. we picked the two strongest rotations, since the performance difference between NPTNs and CNNs was the biggest there and performed our own experiments, adding also RotNets for comparison.
The Paper did specify the data preprocessing in detail, but didn't give new instructions for learning rate, so we kept the same experiment setup as in the first experiment.
The results from our experiments are shown in Figure \ref{pic:second_experiment}. [TODO: new graphic?]


\section{Three-layered-networks}
This experiment from the paper compares three-layered NPTNs and CNNs of the CIFAR10 dataset [CITE]. 
The network dimensions are not described well and the paper states that the amount of parameters for those networks is similar, but not exactly the same as in the previous experiments. 
Therefore we determined possible dimensions for the amount of channels in the NPTNs and CNNs. 
Again no new learning rate was given for this experiment, so we initially tried with the same experimental setup as in the first two experiments, but using this and the dimensions that seemed likely from the paper led the networks to overfit greatly and the loss values did not improve. 
Therefore we reduced the amount of parameters and the learning rate and achieved a result, which had less generalization error. 
Different than in the first two experiments accuracy and loss are not consistent anymore. While the loss starts getting worse after some iteration, the accuracy stays approximately stable and we therefore used the accuracy to compare the networks.
[TODO show results]

\section{Experiments for RotNet}
The final experiment was designed by us to see if RotNet was able to perform one of the ideas behind its design: the ability to be invariant to rotations without learning them. For this we copied the setup of the MNIST rotation experiment with the difference, that this time we only rotated the test set. The results are shown in Figure [FIG]. 


\section{Conclusion}
TRAIN vs TEST ERROR?
CONCLUSION: PAPER IS NOT REPRODUCABLE NPTNs FROM THIS
EXPERIMENTS NOT BETTER ( ROTNET BAD
\section{Discussion and further work}
WHY WE THINK NPTNS ARE LESS POWERFULL WHY ROTNETS ARE
WORSE THAN NPTNs WHY IT IS MAYBE NOT A GOOD IDEA TO RO-
TATE EARLY FILTERs
%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
\bibitem[1980]{2clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

%TODO TO ADD
% - The pyTorch documentation website?
% - The main paper on NPTNs
% - paper from where we took the convolution graphic.

\end{thebibliography}
%\clearpage
%\addtocmark[2]{Author Index} % additional numbered TOC entry
%\renewcommand{\indexname}{Author Index}
%\printindex
%\clearpage
%\addtocmark[2]{Subject Index} % additional numbered TOC entry
%\markboth{Subject Index}{Subject Index}
%\renewcommand{\indexname}{Subject Index}
%\input{subjidx.tex}
\end{document}
